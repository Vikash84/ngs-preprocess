// Long Reads Parameters

/*
      Oxford Nanopore Reads - ONLY

      Loading ONT fastq files
*/

lreads = (params.longReads && params.run_longreads_pipeline && params.lreads_type == 'nanopore') ?
         Channel.fromPath(params.longReads) : Channel.empty()

/*

      PacificBiosciences data - ONLY

      Loading pacbio subreads.bam files

*/

if (params.pacbio_bamPath && params.run_longreads_pipeline && params.lreads_type == 'pacbio') {
    // loading subreads.bam

    bamfiles = Channel.fromPath(params.pacbio_bamPath)
    pacbio_fastq = Channel.empty()

    // loading pacbio fastqs

  } else if (params.longReads && params.run_longreads_pipeline && params.lreads_type == 'pacbio') {
    pacbio_fastq = Channel.fromPath(params.longReads)
    bamfiles = Channel.empty()

    // not loading

  } else {
    bamfiles = Channel.empty()
    pacbio_fastq = Channel.empty()
}

/*

      Loading Pacbio Legacy .bas.h5 data

*/
if (params.lreads_type == 'pacbio' && params.pacbio_h5Path && params.run_longreads_pipeline) {
  String file = params.pacbio_h5Path;
  def h5_name = file.minus(".bas.h5");
  def h5_bax = h5_name + "*.bax.h5"
  println h5_bax ;
  h5bax = Channel.fromPath(h5_bax).collect() ;
  h5files = Channel.fromPath(params.pacbio_h5Path)
} else {
  h5bax = Channel.empty()
  h5files = Channel.empty()
}

/*

          OPTIONAL STEP - FLASH READ MERGE

*/

// Checks if lighter was executed
flash_input = (params.lighter_execute) ? Channel.empty().mix(lighter_corrected_paired_reads)
                                      : Channel.empty().mix(galore_trimmed_paired_reads)

process flashMerger {
  publishDir "${outdir}/illumina", mode: 'copy',
       saveAs: {filename ->
  // This line saves the files with specific sufixes in specific folders
         if (filename.indexOf(".fastq") > 0) "fastqs/flash_merged/$filename"
         else "fastqs/flash_merged/$filename" }
  // This line loads the docker container needed
  container 'fmalmeida/ngs-preprocess'
  tag "Executing FLASH read merger with paired end reads."

  input:
  set val(id), file(sread1), file(sread2) from flash_input

  output:
  file "flash_out*"

  when:
  // Execute this process only when desired and with paired end short reads.
  params.flash_execute && params.run_shortreads_pipeline

  script:
  """
  source activate flash ;
  flash -q -o flash_out -z -t ${params.threads} $sread1 $sread2 &> flash.log;
  """

}

/*
 *      Second Block - Long reads pre-processing
 */

/*
 *      STEP 1 - Extract fasta from h5 files - PACBIO ONLY
 *      Only if user gives h5 data.
 */

process subreadsBamToFastq {
  publishDir "${outdir}/pacbio", mode: 'copy'
  // Loads the necessary Docker image
  container 'fmalmeida/ngs-preprocess'
  tag "Extracting FASTQ from pacbio .subreads.bam files"

  input:
  file(input) from bamfiles

  output:
  file "*.fastq" into pacbio_trimmed mode flatten

  when:
  // Sets execution condition. Only when user have pacbio bam data.
  params.lreads_type == 'pacbio' && params.pacbio_bamPath && params.run_longreads_pipeline

  script:
  if (params.lreads_is_barcoded)
  """
  pbindex ${input} ;
  source activate pbtools ;
  bam2fastq -o ${input.baseName} -u --split-barcodes ${input} ;
  """
  else
  """
  pbindex ${input} ;
  source activate pbtools ;
  bam2fastq -o ${input.baseName} -u ${input}
  """
}

process legacyH5ToFastq {
  publishDir "${outdir}/pacbio", mode: 'copy'
  // Loads the necessary Docker image
  container 'fmalmeida/ngs-preprocess'
  tag "Extracting FASTQ from h5 files"

  input:
  file(inputh5) from h5files
  file h5bax

  output:
  file "${inputh5.baseName}.fastq" into pacbio_legacy_extracted mode flatten

  when:
  // Sets execution condition. Only when user have pacbio h5 data.
  params.lreads_type == 'pacbio' && params.pacbio_h5Path && params.run_longreads_pipeline

  script:
  """
  bash5tools.py --outFilePrefix ${inputh5.baseName} --readType subreads \
  --outType fastq --minLength 200 ${inputh5} ;
  """
}

/*

        STEP 2 - Trimming long reads

*/

process porechopTrimming {
  publishDir "${outdir}/ONT", mode: 'copy'
  // Loads the necessary Docker image
  container 'fmalmeida/ngs-preprocess'
  tag "Trimming with Porechop"

  input:
  file reads from lreads

  output:
  file "${reads.baseName}_trimmed.fastq" into ont_normal_trimmed mode flatten optional true
  file "porechop_barcodes/*.fastq" into ont_barcodes_trimmed mode flatten optional true

  when:
  // Sets execution condition.
  params.lreads_type == 'nanopore' && params.run_longreads_pipeline

  script:
  if (params.lreads_is_barcoded)
  """
  porechop -i ${reads} -b porechop_barcodes --barcode_threshold 85
  """
  else
  """
  porechop -i ${reads} -t ${params.threads} --format fastq -o ${reads.baseName}_trimmed.fastq ;
  """
}

// Create nanopack input channel

ont_trimmed_reads = Channel.empty().mix(ont_normal_trimmed, ont_barcodes_trimmed)
nanopack_input = Channel.empty().mix(pacbio_fastq, ont_trimmed_reads, pacbio_trimmed, pacbio_legacy_extracted)

/*

        STEP 3 - Getting long reads Sequencing quality

*/

process nanopack {
  publishDir outdir, mode: 'copy'
  // Loads the necessary Docker image
  container 'fmalmeida/ngs-preprocess'
  tag "Checking ONT reads stats with NanoPack"

  input:
  file input from nanopack_input

  output:
  file "*"

  when:
  // Sets execution condition. Only when user wants porechop trimming
  params.run_longreads_pipeline

  script:
  """
  source activate nanopack;
  # Plotting
  NanoPlot -t 3 --fastq $input -o ${input.baseName}_nanoplot -f svg --N50 \
  --title "Kp31 sample" --plots hex dot pauvre kde ;

  # Checking Quality
  nanoQC -o ${input.baseName}_nanoQC $input ;

  # Generate Statistics Summary
  NanoStat --fastq $input -n ${input.baseName}.txt --outdir ${input.baseName}_stats ;
  """
}

// Completition message

workflow.onComplete {
    println "Pipeline completed at: $workflow.complete"
    println "Execution status: ${ workflow.success ? 'OK' : 'failed' }"
    println "Execution duration: $workflow.duration"
}
