
process subreadsBamToFastq {
  publishDir "${outdir}/pacbio", mode: 'copy'
  // Loads the necessary Docker image
  container 'fmalmeida/ngs-preprocess'
  tag "Extracting FASTQ from pacbio .subreads.bam files"

  input:
  file(input) from bamfiles

  output:
  file "*.fastq" into pacbio_trimmed mode flatten

  when:
  // Sets execution condition. Only when user have pacbio bam data.
  params.lreads_type == 'pacbio' && params.pacbio_bamPath && params.run_longreads_pipeline

  script:
  if (params.lreads_is_barcoded)
  """
  pbindex ${input} ;
  source activate pbtools ;
  bam2fastq -o ${input.baseName} -u --split-barcodes ${input} ;
  """
  else
  """
  pbindex ${input} ;
  source activate pbtools ;
  bam2fastq -o ${input.baseName} -u ${input}
  """
}

process legacyH5ToFastq {
  publishDir "${outdir}/pacbio", mode: 'copy'
  // Loads the necessary Docker image
  container 'fmalmeida/ngs-preprocess'
  tag "Extracting FASTQ from h5 files"

  input:
  file(inputh5) from h5files
  file h5bax

  output:
  file "${inputh5.baseName}.fastq" into pacbio_legacy_extracted mode flatten

  when:
  // Sets execution condition. Only when user have pacbio h5 data.
  params.lreads_type == 'pacbio' && params.pacbio_h5Path && params.run_longreads_pipeline

  script:
  """
  bash5tools.py --outFilePrefix ${inputh5.baseName} --readType subreads \
  --outType fastq --minLength 200 ${inputh5} ;
  """
}
// Completition message

workflow.onComplete {
    println "Pipeline completed at: $workflow.complete"
    println "Execution status: ${ workflow.success ? 'OK' : 'failed' }"
    println "Execution duration: $workflow.duration"
}
